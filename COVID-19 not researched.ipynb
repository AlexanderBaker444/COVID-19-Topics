{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\603766\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#import all of the needed libraries\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# spacy for lemmatization\n",
    "import spacy\n",
    "\n",
    "# Plotting tools\n",
    "#import pyLDAvis\n",
    "#import pyLDAvis.gensim  # don't skip this\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Enable logging for gensim - optional\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n",
    "\n",
    "#important libraries for NLP\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n",
    "from pytorch_transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
    "from pytorch_transformers import AdamW\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
    "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n",
    "from tqdm import tqdm, trange\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 're', 'edu', 'use'])\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('../data/metadata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45774, 17)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cord_uid</th>\n",
       "      <th>sha</th>\n",
       "      <th>source_x</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>pmcid</th>\n",
       "      <th>pubmed_id</th>\n",
       "      <th>license</th>\n",
       "      <th>abstract</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>authors</th>\n",
       "      <th>journal</th>\n",
       "      <th>Microsoft Academic Paper ID</th>\n",
       "      <th>WHO #Covidence</th>\n",
       "      <th>has_full_text</th>\n",
       "      <th>full_text_file</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vho70jcx</td>\n",
       "      <td>f056da9c64fbf00a4645ae326e8a4339d015d155</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>SIANN: Strain Identification by Alignment to N...</td>\n",
       "      <td>10.1101/001727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Next-generation sequencing is increasingly bei...</td>\n",
       "      <td>2014-01-10</td>\n",
       "      <td>Samuel Minot; Stephen D Turner; Krista L Ternu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9tbix2v</td>\n",
       "      <td>daf32e013d325a6feb80e83d15aabc64a48fae33</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Spatial epidemiology of networked metapopulati...</td>\n",
       "      <td>10.1101/003889</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>An emerging disease is one infectious epidemic...</td>\n",
       "      <td>2014-06-04</td>\n",
       "      <td>Lin WANG; Xiang Li</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/003889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62gfisc6</td>\n",
       "      <td>f33c6d94b0efaa198f8f3f20e644625fa3fe10d2</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Sequencing of the human IG light chain loci fr...</td>\n",
       "      <td>10.1101/006866</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Germline variation at immunoglobulin gene (IG)...</td>\n",
       "      <td>2014-07-03</td>\n",
       "      <td>Corey T Watson; Karyn Meltz Steinberg; Tina A ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/006866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>058r9486</td>\n",
       "      <td>4da8a87e614373d56070ed272487451266dce919</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Bayesian mixture analysis for metagenomic comm...</td>\n",
       "      <td>10.1101/007476</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Deep sequencing of clinical samples is now an ...</td>\n",
       "      <td>2014-07-25</td>\n",
       "      <td>Sofia Morfopoulou; Vincent Plagnol</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/007476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wich35l7</td>\n",
       "      <td>eccef80cfbe078235df22398f195d5db462d8000</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Mapping a viral phylogeny onto outbreak trees ...</td>\n",
       "      <td>10.1101/010389</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>biorxiv</td>\n",
       "      <td>Developing methods to reconstruct transmission...</td>\n",
       "      <td>2014-11-11</td>\n",
       "      <td>Stephen P Velsko; Jonathan E Allen</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>biorxiv_medrxiv</td>\n",
       "      <td>https://doi.org/10.1101/010389</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cord_uid                                       sha source_x  \\\n",
       "0  vho70jcx  f056da9c64fbf00a4645ae326e8a4339d015d155  biorxiv   \n",
       "1  i9tbix2v  daf32e013d325a6feb80e83d15aabc64a48fae33  biorxiv   \n",
       "2  62gfisc6  f33c6d94b0efaa198f8f3f20e644625fa3fe10d2  biorxiv   \n",
       "3  058r9486  4da8a87e614373d56070ed272487451266dce919  biorxiv   \n",
       "4  wich35l7  eccef80cfbe078235df22398f195d5db462d8000  biorxiv   \n",
       "\n",
       "                                               title             doi pmcid  \\\n",
       "0  SIANN: Strain Identification by Alignment to N...  10.1101/001727   NaN   \n",
       "1  Spatial epidemiology of networked metapopulati...  10.1101/003889   NaN   \n",
       "2  Sequencing of the human IG light chain loci fr...  10.1101/006866   NaN   \n",
       "3  Bayesian mixture analysis for metagenomic comm...  10.1101/007476   NaN   \n",
       "4  Mapping a viral phylogeny onto outbreak trees ...  10.1101/010389   NaN   \n",
       "\n",
       "   pubmed_id  license                                           abstract  \\\n",
       "0        NaN  biorxiv  Next-generation sequencing is increasingly bei...   \n",
       "1        NaN  biorxiv  An emerging disease is one infectious epidemic...   \n",
       "2        NaN  biorxiv  Germline variation at immunoglobulin gene (IG)...   \n",
       "3        NaN  biorxiv  Deep sequencing of clinical samples is now an ...   \n",
       "4        NaN  biorxiv  Developing methods to reconstruct transmission...   \n",
       "\n",
       "  publish_time                                            authors journal  \\\n",
       "0   2014-01-10  Samuel Minot; Stephen D Turner; Krista L Ternu...     NaN   \n",
       "1   2014-06-04                                 Lin WANG; Xiang Li     NaN   \n",
       "2   2014-07-03  Corey T Watson; Karyn Meltz Steinberg; Tina A ...     NaN   \n",
       "3   2014-07-25                 Sofia Morfopoulou; Vincent Plagnol     NaN   \n",
       "4   2014-11-11                 Stephen P Velsko; Jonathan E Allen     NaN   \n",
       "\n",
       "   Microsoft Academic Paper ID WHO #Covidence  has_full_text   full_text_file  \\\n",
       "0                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "1                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "2                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "3                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "4                          NaN            NaN           True  biorxiv_medrxiv   \n",
       "\n",
       "                              url  \n",
       "0  https://doi.org/10.1101/001727  \n",
       "1  https://doi.org/10.1101/003889  \n",
       "2  https://doi.org/10.1101/006866  \n",
       "3  https://doi.org/10.1101/007476  \n",
       "4  https://doi.org/10.1101/010389  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data cleaning dedup on abstract\n",
    "#how orignial the abstract is\n",
    "#based on the references and resources the author created the \n",
    "#how similar the paper is to the connecting points\n",
    "#originality score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstracts=df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clean the documents\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop1 = set(stopwords.words('english'))\n",
    "stop2 = set(stopwords.words('spanish'))\n",
    "stop3 = set(stopwords.words('french'))\n",
    "stop=[stop1,stop2,stop3]\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "str_abstract=[str(abstract) for abstract in df['abstract']]\n",
    "abstract_array =[clean(abstract).split() for abstract in str_abstract]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#join the cleaned abstracts\n",
    "abstract_array_joined =[\" \".join(clean(abstract).split()) for abstract in str_abstract]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = abstract_array\n",
    "new_sent=[]\n",
    "for i in sentences:\n",
    "    new_sent.append(str(i))\n",
    "sentences = [\"[SEP]\"+sentence +\"[CLS]\" for sentence in new_sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁[', 's', 'ep', ']', '[', \"'\", 'next', 'generation', \"'\", ',', '▁', \"'\", 'se', 'qu', 'encing', \"'\", ',', '▁', \"'\", 'is', \"'\", ',', '▁', \"'\", 'increasing', 'ly', \"'\", ',', '▁', \"'\", 'being', \"'\", ',', '▁', \"'\", 'used', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'stud', 'y', \"'\", ',', '▁', \"'\", 's', 'amp', 'le', \"'\", ',', '▁', \"'\", 'com', 'posed', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'mix', 'ture', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'organ', 'ism', \"'\", ',', '▁', \"'\", 'such', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'in', \"'\", ',', '▁', \"'\", 'clin', 'ical', \"'\", ',', '▁', \"'\", 'application', \"'\", ',', '▁', \"'\", 'where', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'pre', 's', 'ence', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'path', 'ogen', \"'\", ',', '▁', \"'\", 'at', \"'\", ',', '▁', \"'\", 'very', \"'\", ',', '▁', \"'\", 'low', \"'\", ',', '▁', \"'\", 'a', 'bund', 'ance', \"'\", ',', '▁', \"'\", 'may', \"'\", ',', '▁', \"'\", 'be', \"'\", ',', '▁', \"'\", 'high', 'ly', \"'\", ',', '▁', \"'\", 'important', \"'\", ',', '▁', \"'\", 'we', \"'\", ',', '▁', \"'\", 'present', \"'\", ',', '▁', \"'\", 'an', \"'\", ',', '▁', \"'\", 'analytic', 'al', \"'\", ',', '▁', \"'\", 'meth', 'od', \"'\", ',', '▁', \"'\", 's', 'ian', 'n', \"'\", ',', '▁', \"'\", 's', 'train', \"'\", ',', '▁', \"'\", 'ident', 'ification', \"'\", ',', '▁', \"'\", 'by', \"'\", ',', '▁', \"'\", 'align', 'ment', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'near', \"'\", ',', '▁', \"'\", 'ne', 'igh', 'bor', \"'\", ',', '▁', \"'\", 'specific', 'ally', \"'\", ',', '▁', \"'\", 'designed', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'rapid', 'ly', \"'\", ',', '▁', \"'\", 'detect', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'set', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'target', \"'\", ',', '▁', \"'\", 'organ', 'ism', \"'\", ',', '▁', \"'\", 'in', \"'\", ',', '▁', \"'\", 'mix', 'ed', \"'\", ',', '▁', \"'\", 's', 'amp', 'le', \"'\", ',', '▁', \"'\", 'that', \"'\", ',', '▁', \"'\", 'achi', 'eves', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'high', \"'\", ',', '▁', \"'\", 'degree', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 's', 'pec', 'ie', \"'\", ',', '▁', \"'\", 'and', \"'\", ',', '▁', \"'\", 's', 'train', 'specific', 'ity', \"'\", ',', '▁', \"'\", 'by', \"'\", ',', '▁', \"'\", 'align', 'ing', \"'\", ',', '▁', \"'\", 'short', \"'\", ',', '▁', \"'\", 'sequence', \"'\", ',', '▁', \"'\", 'read', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'gen', 'ome', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'near', \"'\", ',', '▁', \"'\", 'ne', 'igh', 'bor', \"'\", ',', '▁', \"'\", 'organ', 'ism', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'well', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'that', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'target', \"'\", ',', '▁', \"'\", 'emp', 'ir', 'ical', \"'\", ',', '▁', \"'\", 'bench', 'mark', 'ing', \"'\", ',', '▁', \"'\", 'along', 'side', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'current', \"'\", ',', '▁', \"'\", 'state', 'of', 't', 'heart', \"'\", ',', '▁', \"'\", 'meth', 'od', \"'\", ',', '▁', \"'\", 'show', \"'\", ',', '▁', \"'\", 'an', \"'\", ',', '▁', \"'\", 'extremely', \"'\", ',', '▁', \"'\", 'high', \"'\", ',', '▁', \"'\", 'positive', \"'\", ',', '▁', \"'\", 'pre', 'dict', 'ive', \"'\", ',', '▁', \"'\", 'value', \"'\", ',', '▁', \"'\", 'even', \"'\", ',', '▁', \"'\", 'at', \"'\", ',', '▁', \"'\", 'very', \"'\", ',', '▁', \"'\", 'low', \"'\", ',', '▁', \"'\", 'a', 'bund', 'ance', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'target', \"'\", ',', '▁', \"'\", 'organ', 'ism', \"'\", ',', '▁', \"'\", 'in', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'mix', 'ed', \"'\", ',', '▁', \"'\", 's', 'amp', 'le', \"'\", ',', '▁', \"'\", 's', 'ian', 'n', \"'\", ',', '▁', \"'\", 'is', \"'\", ',', '▁', \"'\", 'available', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'an', \"'\", ',', '▁', \"'\", 'il', 'lumina', \"'\", ',', '▁', \"'\", 'base', 'space', \"'\", ',', '▁', \"'\", 'app', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'well', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'through', \"'\", ',', '▁', \"'\", 'signature', \"'\", ',', '▁', \"'\", 'science', \"'\", ',', '▁', \"'\", 'll', 'c', \"'\", ',', '▁', \"'\", 's', 'ian', 'n', \"'\", ',', '▁', \"'\", 're', 's', 'ult', \"'\", ',', '▁', \"'\", 'are', \"'\", ',', '▁', \"'\", 'present', 'ed', \"'\", ',', '▁', \"'\", 'in', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'stream', 'lined', \"'\", ',', '▁', \"'\", 'report', \"'\", ',', '▁', \"'\", 'designed', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'be', \"'\", ',', '▁', \"'\", 'comp', 're', 'hen', 's', 'ible', \"'\", ',', '▁', \"'\", 'to', \"'\", ',', '▁', \"'\", 'the', \"'\", ',', '▁', \"'\", 'non', 'specialist', \"'\", ',', '▁', \"'\", 'user', \"'\", ',', '▁', \"'\", 'pro', 'vid', 'ing', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'power', 'ful', \"'\", ',', '▁', \"'\", 'tool', \"'\", ',', '▁', \"'\", 'for', \"'\", ',', '▁', \"'\", 'rapid', \"'\", ',', '▁', \"'\", 's', 'pec', 'ie', \"'\", ',', '▁', \"'\", 'detect', 'ion', \"'\", ',', '▁', \"'\", 'in', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'mix', 'ed', \"'\", ',', '▁', \"'\", 's', 'amp', 'le', \"'\", ',', '▁', \"'\", 'by', \"'\", ',', '▁', \"'\", 'focus', 'ing', \"'\", ',', '▁', \"'\", 'on', \"'\", ',', '▁', \"'\", 'a', \"'\", ',', '▁', \"'\", 'set', \"'\", ',', '▁', \"'\", 'of', \"'\", ',', '▁', \"'\", 'custom', 'iz', 'able', \"'\", ',', '▁', \"'\", 'target', \"'\", ',', '▁', \"'\", 'organ', 'ism', \"'\", ',', '▁', \"'\", 'and', \"'\", ',', '▁', \"'\", 'their', \"'\", ',', '▁', \"'\", 'near', \"'\", ',', '▁', \"'\", 'ne', 'igh', 'bor', \"'\", ',', '▁', \"'\", 's', 'ian', 'n', \"'\", ',', '▁', \"'\", 'can', \"'\", ',', '▁', \"'\", 'oper', 'ate', \"'\", ',', '▁', \"'\", 'quick', 'ly', \"'\", ',', '▁', \"'\", 'and', \"'\", ',', '▁', \"'\", 'with', \"'\", ',', '▁', \"'\", 'low', \"'\", ',', '▁', \"'\", 'com', 'put', 'ational', \"'\", ',', '▁', \"'\", 're', 'qui', 're', 'ment', \"'\", ',', '▁', \"'\", 'while', \"'\", ',', '▁', \"'\", 'deliver', 'ing', \"'\", ',', '▁', \"'\", 'high', 'ly', \"'\", ',', '▁', \"'\", 'accu', 'rate', \"'\", ',', '▁', \"'\", 're', 's', 'ult', \"'\", ']', '[', 'cl', 's', ']']\n"
     ]
    }
   ],
   "source": [
    "#research XLNet a little more, make sure implemntation is correct\n",
    "tokenizer_2 = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
    "tokenized_texts_2 = [tokenizer_2.tokenize(sent) for sent in sentences]\n",
    "print(tokenized_texts_2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_2 = [tokenizer_2.convert_tokens_to_ids(x) for x in tokenized_texts_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_2 = pad_sequences(input_ids_2, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4145    23  3882  3158 10849    26  2371    98   814    26    19    17\n",
      "    26  8588   773    26    19    17    26   627    26    19    17    26\n",
      " 26729  7061 12801 22148    26    19    17    26 14466    26    19    17\n",
      "    26  3033    26    19    17    26 12579   415    26    19    17    26\n",
      "   590    26    19    17    26 18479    26    19    17    26  1383    26\n",
      "    19    17    26  9082 11406 24353    26    19    17    26 26729   769\n",
      "    26    19    17    26  2438    26    19    17    26 17283    56    26\n",
      "    19    17    26 16258    26    19    17    26 25026    26    19    17\n",
      "    26 30906    26    19    17    26   153    26    19    17    26 24617\n",
      "    26    19    17    26 23467    26    19    17    26   590    26    19\n",
      "    17    26 31664 11809    26    19    17    26 11730    26    19    17\n",
      "    26  1168    26    19    17    26    23  4123 18898    26    19    17\n",
      "    26 27424    26    19    17    26]\n"
     ]
    }
   ],
   "source": [
    "print(input_ids_2[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction PCA UMAP TSNE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple kmeans clustering \n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=10, random_state=0).fit(input_ids_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45774"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kmeans.labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hierarchal clustering \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg=AgglomerativeClustering(n_clusters=10).fit(input_ids_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, ..., 4, 7, 1], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering algorithm 3\n",
    "from sklearn.cluster import DBSCAN\n",
    "db=DBSCAN(eps=3).fit(input_ids_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gaussian Mixture Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, ..., -1,  0, -1], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new dataframe\n",
    "d = {'id':df['cord_uid'] , 'cluster': agg.labels_,'abstract':abstract_array_joined}\n",
    "clusters=pd.DataFrame(data=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vho70jcx</td>\n",
       "      <td>1</td>\n",
       "      <td>nextgeneration sequencing is increasingly bein...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i9tbix2v</td>\n",
       "      <td>0</td>\n",
       "      <td>an emerging disease is one infectious epidemic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62gfisc6</td>\n",
       "      <td>3</td>\n",
       "      <td>germline variation at immunoglobulin gene ig l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>058r9486</td>\n",
       "      <td>0</td>\n",
       "      <td>deep sequencing of clinical sample is now an e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wich35l7</td>\n",
       "      <td>3</td>\n",
       "      <td>developing method to reconstruct transmission ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  cluster                                           abstract\n",
       "0  vho70jcx        1  nextgeneration sequencing is increasingly bein...\n",
       "1  i9tbix2v        0  an emerging disease is one infectious epidemic...\n",
       "2  62gfisc6        3  germline variation at immunoglobulin gene ig l...\n",
       "3  058r9486        0  deep sequencing of clinical sample is now an e...\n",
       "4  wich35l7        3  developing method to reconstruct transmission ..."
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clusters.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "gf=clusters.groupby('cluster').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10660</td>\n",
       "      <td>10660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3181</td>\n",
       "      <td>3181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1189</td>\n",
       "      <td>1189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8353</td>\n",
       "      <td>8353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5915</td>\n",
       "      <td>5915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2970</td>\n",
       "      <td>2970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>943</td>\n",
       "      <td>943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8199</td>\n",
       "      <td>8199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1716</td>\n",
       "      <td>1716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2648</td>\n",
       "      <td>2648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  abstract\n",
       "cluster                 \n",
       "0        10660     10660\n",
       "1         3181      3181\n",
       "2         1189      1189\n",
       "3         8353      8353\n",
       "4         5915      5915\n",
       "5         2970      2970\n",
       "6          943       943\n",
       "7         8199      8199\n",
       "8         1716      1716\n",
       "9         2648      2648"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gf\n",
    "#try a double dbscan, dbscan once, then dbscan the first cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster1=clusters[clusters['cluster']==-1].dropna()\n",
    "cluster2=clusters[clusters['cluster']==0].dropna()\n",
    "cluster3=clusters[clusters['cluster']==1].dropna()\n",
    "cluster4=clusters[clusters['cluster']==2].dropna()\n",
    "cluster5=clusters[clusters['cluster']==3].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#seperate the clusters into their own dataframes\n",
    "cluster1=clusters[clusters['cluster']==0].dropna()\n",
    "cluster2=clusters[clusters['cluster']==1].dropna()\n",
    "cluster3=clusters[clusters['cluster']==2].dropna()\n",
    "cluster4=clusters[clusters['cluster']==3].dropna()\n",
    "cluster5=clusters[clusters['cluster']==4].dropna()\n",
    "cluster6=clusters[clusters['cluster']==5].dropna()\n",
    "cluster7=clusters[clusters['cluster']==6].dropna()\n",
    "cluster8=clusters[clusters['cluster']==7].dropna()\n",
    "cluster9=clusters[clusters['cluster']==8].dropna()\n",
    "cluster10=clusters[clusters['cluster']==9].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find the frequency of topics \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "no_features = 1000\n",
    "\n",
    "# NMF is able to use tf-idf\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tfidf = tfidf_vectorizer.fit_transform(cluster7['abstract'])\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2, max_features=no_features, stop_words='english')\n",
    "tf = tf_vectorizer.fit_transform(cluster7['abstract'])\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#topic model\n",
    "#always more types of topic modeling Latent Discriminate Analysis \n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "no_topics = 10\n",
    "\n",
    "# Run NMF\n",
    "nmf = NMF(n_components=no_topics, random_state=1, alpha=.1, l1_ratio=.5, init='nndsvd').fit(tfidf)\n",
    "\n",
    "# Run LDA\n",
    "lda = LatentDirichletAllocation( max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: respiratory child sample infection wa assay virus detected pathogen detection\n",
      "Topic #1: rna sequence genome viral gene virus replication region dna coronavirus\n",
      "Topic #2: health disease outbreak infectious public infection risk case ha data\n",
      "Topic #3: cell mouse infection expression immune receptor viral response cytokine virus\n",
      "Topic #4: protein membrane domain fusion peptide activity lipid structure cellular binding\n",
      "Topic #5: antibody monoclonal neutralizing epitope serum igg antigen assay produced used\n",
      "Topic #6: virus influenza human h5n1 avian h1n1 pathogenic acid pandemic detection\n",
      "Topic #7: sars sarscov 2003 severe syndrome cov acute respiratory woman kong\n",
      "Topic #8: vaccine immune vaccination response safety development antigen hajj efficacy strategy\n",
      "Topic #9: patient group pneumonia day wa covid19 treatment case symptom hospital\n",
      "\n",
      "Topic #0: sars le la et cov en unit agent 2003 epidemic\n",
      "Topic #1: vaccine disease immune response study vaccination milk review abstract ha\n",
      "Topic #2: protein rna activity sequence coronavirus region wa gene study sarscov\n",
      "Topic #3: pedv hbov1 facility collected surface rna application production swab sample\n",
      "Topic #4: patient wa respiratory group sample virus clinical assay result detection\n",
      "Topic #5: drug disease asthma viral virus development lung treatment clinical ha\n",
      "Topic #6: cell virus viral wa infection protein human abstract expression replication\n",
      "Topic #7: virus respiratory wa infection child study patient influenza viral symptom\n",
      "Topic #8: wa health study case infection disease patient outbreak method result\n",
      "Topic #9: component natural chinese serum experiment used molecular procedure presented study\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#display topics\n",
    "#look up methods for word patterns and frequncy analysis \n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()\n",
    "\n",
    "no_top_words = 10\n",
    "print_top_words(nmf, tfidf_feature_names, no_top_words)\n",
    "print_top_words(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
